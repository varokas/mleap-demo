{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fcf858b-7eb2-4329-beac-0d3a92b5dba4",
   "metadata": {},
   "source": [
    "# AirBNB "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff5dafd-25bf-4eef-b16a-e639ba353825",
   "metadata": {},
   "source": [
    "Adapted from Demo here\n",
    "* https://github.com/combust/mleap-demo/blob/master/notebooks/PySpark%20-%20AirBnb.ipynb\n",
    "* https://combust.github.io/mleap-docs/py-spark/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83831559-4153-4a4b-893e-d1a72400c30c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File ‘airbnb.csv’ already there; not retrieving.\n"
     ]
    }
   ],
   "source": [
    "# download sample data file\n",
    "!wget -nc -O airbnb.csv https://github.com/combust/mleap-demo/blob/master/data/airbnb.csv?raw=true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abed6f27-c4c1-4c18-9a12-553ff82e710c",
   "metadata": {},
   "source": [
    "## Import MLeap from PIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78bd3450-093a-457e-842c-7feb4f940b70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mleap==0.18.0\n",
      "  Using cached mleap-0.18.0-py3-none-any.whl (59 kB)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.9/site-packages (0.24.2)\n",
      "Installing collected packages: mleap\n",
      "Successfully installed mleap-0.18.0\n"
     ]
    }
   ],
   "source": [
    "!pip install mleap==0.18.0 --no-deps scikit-learn\n",
    "\n",
    "#mleap==0.18.0 requires, scikit-learn<0.23.0,>=0.22.0 -- but has 0.24.0 installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e19fbce-5cdf-40bc-b0ce-3aef1bda06c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### !!! IMPORTANT: Uses pyspark from mleap package !!! ###\n",
    "\n",
    "from mleap import pyspark\n",
    "from mleap.pyspark.spark_support import SimpleSparkSerializer\n",
    "\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler, OneHotEncoder, StringIndexer\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a93927d3-bb2b-4c75-9b3f-acc438352120",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.1.2-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Ivy Default Cache set to: /home/jovyan/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jovyan/.ivy2/jars\n",
      "ml.combust.mleap#mleap-spark_2.12 added as a dependency\n",
      "ml.combust.mleap#mleap-runtime_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-8bfe831d-54cf-4b1c-b8de-6261ec698abf;1.0\n",
      "\tconfs: [default]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/spark-3.1.2-bin-hadoop3.2/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound ml.combust.mleap#mleap-spark_2.12;0.18.0 in central\n",
      "\tfound ml.combust.mleap#mleap-spark-base_2.12;0.18.0 in central\n",
      "\tfound ml.combust.mleap#mleap-runtime_2.12;0.18.0 in central\n",
      "\tfound ml.combust.mleap#mleap-core_2.12;0.18.0 in central\n",
      "\tfound ml.combust.mleap#mleap-base_2.12;0.18.0 in central\n",
      "\tfound ml.combust.mleap#mleap-tensor_2.12;0.18.0 in central\n",
      "\tfound io.spray#spray-json_2.12;1.3.2 in central\n",
      "\tfound com.github.rwl#jtransforms;2.4.0 in central\n",
      "\tfound org.scalanlp#breeze_2.12;1.0 in central\n",
      "\tfound org.scalanlp#breeze-macros_2.12;1.0 in central\n",
      "\tfound com.github.fommil.netlib#core;1.1.2 in central\n",
      "\tfound net.sourceforge.f2j#arpack_combined_all;0.1 in central\n",
      "\tfound net.sf.opencsv#opencsv;2.3 in central\n",
      "\tfound com.github.wendykierp#JTransforms;3.1 in central\n",
      "\tfound pl.edu.icm#JLargeArrays;1.5 in central\n",
      "\tfound org.apache.commons#commons-math3;3.2 in central\n",
      "\tfound com.chuusai#shapeless_2.12;2.3.3 in central\n",
      "\tfound org.typelevel#macro-compat_2.12;1.1.1 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.5 in central\n",
      "\tfound org.typelevel#spire_2.12;0.17.0-M1 in central\n",
      "\tfound org.typelevel#spire-macros_2.12;0.17.0-M1 in central\n",
      "\tfound org.typelevel#machinist_2.12;0.6.8 in central\n",
      "\tfound org.typelevel#algebra_2.12;2.0.0-M2 in central\n",
      "\tfound org.typelevel#cats-kernel_2.12;2.0.0-M4 in central\n",
      "\tfound org.typelevel#spire-platform_2.12;0.17.0-M1 in central\n",
      "\tfound org.typelevel#spire-util_2.12;0.17.0-M1 in central\n",
      "\tfound org.scala-lang.modules#scala-collection-compat_2.12;2.1.1 in central\n",
      "\tfound ml.combust.bundle#bundle-ml_2.12;0.18.0 in central\n",
      "\tfound com.google.protobuf#protobuf-java;3.5.1 in central\n",
      "\tfound com.thesamet.scalapb#scalapb-runtime_2.12;0.7.1 in central\n",
      "\tfound com.thesamet.scalapb#lenses_2.12;0.7.0-test2 in central\n",
      "\tfound com.lihaoyi#fastparse_2.12;1.0.0 in central\n",
      "\tfound com.lihaoyi#fastparse-utils_2.12;1.0.0 in central\n",
      "\tfound com.lihaoyi#sourcecode_2.12;0.1.4 in central\n",
      "\tfound com.jsuereth#scala-arm_2.12;2.0 in central\n",
      "\tfound com.typesafe#config;1.3.0 in central\n",
      "\tfound commons-io#commons-io;2.5 in central\n",
      "\tfound org.scala-lang#scala-reflect;2.12.13 in central\n",
      "\tfound ml.combust.bundle#bundle-hdfs_2.12;0.18.0 in central\n",
      ":: resolution report :: resolve 1358ms :: artifacts dl 252ms\n",
      "\t:: modules in use:\n",
      "\tcom.chuusai#shapeless_2.12;2.3.3 from central in [default]\n",
      "\tcom.github.fommil.netlib#core;1.1.2 from central in [default]\n",
      "\tcom.github.rwl#jtransforms;2.4.0 from central in [default]\n",
      "\tcom.github.wendykierp#JTransforms;3.1 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java;3.5.1 from central in [default]\n",
      "\tcom.jsuereth#scala-arm_2.12;2.0 from central in [default]\n",
      "\tcom.lihaoyi#fastparse-utils_2.12;1.0.0 from central in [default]\n",
      "\tcom.lihaoyi#fastparse_2.12;1.0.0 from central in [default]\n",
      "\tcom.lihaoyi#sourcecode_2.12;0.1.4 from central in [default]\n",
      "\tcom.thesamet.scalapb#lenses_2.12;0.7.0-test2 from central in [default]\n",
      "\tcom.thesamet.scalapb#scalapb-runtime_2.12;0.7.1 from central in [default]\n",
      "\tcom.typesafe#config;1.3.0 from central in [default]\n",
      "\tcommons-io#commons-io;2.5 from central in [default]\n",
      "\tio.spray#spray-json_2.12;1.3.2 from central in [default]\n",
      "\tml.combust.bundle#bundle-hdfs_2.12;0.18.0 from central in [default]\n",
      "\tml.combust.bundle#bundle-ml_2.12;0.18.0 from central in [default]\n",
      "\tml.combust.mleap#mleap-base_2.12;0.18.0 from central in [default]\n",
      "\tml.combust.mleap#mleap-core_2.12;0.18.0 from central in [default]\n",
      "\tml.combust.mleap#mleap-runtime_2.12;0.18.0 from central in [default]\n",
      "\tml.combust.mleap#mleap-spark-base_2.12;0.18.0 from central in [default]\n",
      "\tml.combust.mleap#mleap-spark_2.12;0.18.0 from central in [default]\n",
      "\tml.combust.mleap#mleap-tensor_2.12;0.18.0 from central in [default]\n",
      "\tnet.sf.opencsv#opencsv;2.3 from central in [default]\n",
      "\tnet.sourceforge.f2j#arpack_combined_all;0.1 from central in [default]\n",
      "\torg.apache.commons#commons-math3;3.2 from central in [default]\n",
      "\torg.scala-lang#scala-reflect;2.12.13 from central in [default]\n",
      "\torg.scala-lang.modules#scala-collection-compat_2.12;2.1.1 from central in [default]\n",
      "\torg.scalanlp#breeze-macros_2.12;1.0 from central in [default]\n",
      "\torg.scalanlp#breeze_2.12;1.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.5 from central in [default]\n",
      "\torg.typelevel#algebra_2.12;2.0.0-M2 from central in [default]\n",
      "\torg.typelevel#cats-kernel_2.12;2.0.0-M4 from central in [default]\n",
      "\torg.typelevel#machinist_2.12;0.6.8 from central in [default]\n",
      "\torg.typelevel#macro-compat_2.12;1.1.1 from central in [default]\n",
      "\torg.typelevel#spire-macros_2.12;0.17.0-M1 from central in [default]\n",
      "\torg.typelevel#spire-platform_2.12;0.17.0-M1 from central in [default]\n",
      "\torg.typelevel#spire-util_2.12;0.17.0-M1 from central in [default]\n",
      "\torg.typelevel#spire_2.12;0.17.0-M1 from central in [default]\n",
      "\tpl.edu.icm#JLargeArrays;1.5 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\torg.scala-lang#scala-reflect;2.12.8 by [org.scala-lang#scala-reflect;2.12.13] in [default]\n",
      "\torg.apache.commons#commons-math3;3.5 by [org.apache.commons#commons-math3;3.2] in [default]\n",
      "\tcom.google.protobuf#protobuf-java;3.5.0 by [com.google.protobuf#protobuf-java;3.5.1] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   42  |   0   |   0   |   3   ||   39  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-8bfe831d-54cf-4b1c-b8de-6261ec698abf\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 39 already retrieved (0kB/39ms)\n",
      "21/09/19 18:10:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "### !!! IMPORTANT: Import MLeap Jars, or serialization will fail !!! ###\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[1]\") \\\n",
    "                    .appName('mleap') \\\n",
    "                    .config('spark.jars.packages', 'ml.combust.mleap:mleap-spark_2.12:0.18.0,ml.combust.mleap:mleap-runtime_2.12:0.18.0')\\\n",
    "                    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a39d20-f44c-46e3-85fc-04ec5cd938e1",
   "metadata": {},
   "source": [
    "## Prepare Data in Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe8c2da7-ba36-461a-9703-588af444eb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType, DoubleType\n",
    "\n",
    "schema = StructType([ \\\n",
    "    StructField(\"id\",IntegerType(),True), \\\n",
    "    StructField(\"name\",StringType(),True), \\\n",
    "    StructField(\"price\",DoubleType(),True), \\\n",
    "    StructField(\"bedrooms\", DoubleType(), True), \\\n",
    "    StructField(\"bathrooms\", DoubleType(), True), \\\n",
    "    StructField(\"room_type\", StringType(), True), \\\n",
    "    StructField(\"square_feet\", DoubleType(), True), \\\n",
    "    StructField(\"host_is_superhost\", DoubleType(), True), \\\n",
    "    StructField(\"state\", StringType(), True), \\\n",
    "    StructField(\"cancellation_policy\", StringType(), True), \\\n",
    "    StructField(\"security_deposit\", DoubleType(), True), \\\n",
    "    StructField(\"cleaning_fee\", DoubleType(), True), \\\n",
    "    StructField(\"extra_people\", DoubleType(), True), \\\n",
    "    StructField(\"number_of_reviews\", IntegerType(), True), \\\n",
    "    StructField(\"price_per_bedroom\", DoubleType(), True), \\\n",
    "    StructField(\"review_scores_rating\", DoubleType(), True), \\\n",
    "    StructField(\"instant_bookable\", DoubleType(), True) \\\n",
    "])\n",
    " \n",
    "\n",
    "df = spark.read.option(\"header\", True).schema(schema).csv(\"airbnb.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "396b5235-da4b-42f9-8a6b-692648620f17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(id=1949687, name='Delectable Victorian Flat for two', price=80.0, bedrooms=1.0, bathrooms=1.0, room_type='Entire home/apt', square_feet=None, host_is_superhost=0.0, state='London', cancellation_policy='moderate', security_deposit=100.0, cleaning_fee=20.0, extra_people=10.0, number_of_reviews=8, price_per_bedroom=80.0, review_scores_rating=94.0, instant_bookable=0.0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53fceec1-2a5c-4dbd-9a6c-69d59bb7d0ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "389255\n",
      "321588\n"
     ]
    }
   ],
   "source": [
    "datasetFiltered = df.filter(\"price >= 50 AND price <= 750 and bathrooms > 0.0\")\n",
    "print(df.count())\n",
    "print(datasetFiltered.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a125072-f5c0-44e0-99cd-72d716009a93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "datasetFiltered.registerTempTable(\"df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "045e1500-1b30-4913-9bf7-016c0803eae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+------------------+-----------------+\n",
      "|summary|       square_feet|             price|          bedrooms|         bathrooms|     cleaning_fee|\n",
      "+-------+------------------+------------------+------------------+------------------+-----------------+\n",
      "|  count|            321588|            321588|            321588|            321588|           321588|\n",
      "|   mean| 546.7441757777032|131.54961006007687|1.3352426085550455| 1.199068373198005|37.64188340360959|\n",
      "| stddev|363.39839582374066| 90.10912788720098|0.8466586601060778|0.4830590051262673|42.64237791484579|\n",
      "|    min|             104.0|              50.0|               0.0|               0.5|              0.0|\n",
      "|    max|           32292.0|             750.0|              10.0|               8.0|            700.0|\n",
      "+-------+------------------+------------------+------------------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datasetImputed = spark.sql(\"\"\"\n",
    "    select\n",
    "        id,\n",
    "        case when state in('NY', 'CA', 'London', 'Berlin', 'TX' ,'IL', 'OR', 'DC', 'WA')\n",
    "            then state\n",
    "            else 'Other'\n",
    "        end as state,\n",
    "        price,\n",
    "        bathrooms,\n",
    "        bedrooms,\n",
    "        room_type,\n",
    "        host_is_superhost,\n",
    "        cancellation_policy,\n",
    "        case when security_deposit is null\n",
    "            then 0.0\n",
    "            else security_deposit\n",
    "        end as security_deposit,\n",
    "        price_per_bedroom,\n",
    "        case when number_of_reviews is null\n",
    "            then 0.0\n",
    "            else number_of_reviews\n",
    "        end as number_of_reviews,\n",
    "        case when extra_people is null\n",
    "            then 0.0\n",
    "            else extra_people\n",
    "        end as extra_people,\n",
    "        instant_bookable,\n",
    "        case when cleaning_fee is null\n",
    "            then 0.0\n",
    "            else cleaning_fee\n",
    "        end as cleaning_fee,\n",
    "        case when review_scores_rating is null\n",
    "            then 80.0\n",
    "            else review_scores_rating\n",
    "        end as review_scores_rating,\n",
    "        case when square_feet is not null and square_feet > 100\n",
    "            then square_feet\n",
    "            when (square_feet is null or square_feet <=100) and (bedrooms is null or bedrooms = 0)\n",
    "            then 350.0\n",
    "            else 380 * bedrooms\n",
    "        end as square_feet,\n",
    "        case when bathrooms >= 2\n",
    "            then 1.0\n",
    "            else 0.0\n",
    "        end as n_bathrooms_more_than_two\n",
    "    from df\n",
    "    where bedrooms is not null\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "datasetImputed.select(\"square_feet\", \"price\", \"bedrooms\", \"bathrooms\", \"cleaning_fee\").describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2563e9e2-bd86-4a3a-9827-9f561651eea4",
   "metadata": {},
   "source": [
    "## Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d4393e0-c6bc-4f15-849e-84b3c03ba63a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:======================================================>(198 + 1) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+---------+---------+\n",
      "|        state|    n|avg_price|max_price|\n",
      "+-------------+-----+---------+---------+\n",
      "|           NY|48362|   146.75|    750.0|\n",
      "|           CA|44716|   158.76|    750.0|\n",
      "|Île-de-France|40732|   107.74|    750.0|\n",
      "|       London|17542|   117.72|    750.0|\n",
      "|          NSW|14416|   167.96|    750.0|\n",
      "|       Berlin|13098|    81.01|    650.0|\n",
      "|Noord-Holland| 8890|   128.56|    750.0|\n",
      "|          VIC| 8636|   144.49|    750.0|\n",
      "|North Holland| 7636|   134.60|    700.0|\n",
      "|           IL| 7544|   141.85|    750.0|\n",
      "|           ON| 7186|   129.05|    750.0|\n",
      "|           TX| 6702|   196.59|    750.0|\n",
      "|           WA| 5858|   132.48|    750.0|\n",
      "|    Catalonia| 5748|   106.39|    720.0|\n",
      "|           BC| 5522|   133.14|    750.0|\n",
      "|           DC| 5476|   136.56|    720.0|\n",
      "|       Québec| 5116|   104.98|    700.0|\n",
      "|    Catalunya| 4570|    99.36|    675.0|\n",
      "|       Veneto| 4486|   131.71|    700.0|\n",
      "|           OR| 4330|   114.02|    700.0|\n",
      "+-------------+-----+---------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    select \n",
    "        state,\n",
    "        count(*) as n,\n",
    "        cast(avg(price) as decimal(12,2)) as avg_price,\n",
    "        max(price) as max_price\n",
    "    from df\n",
    "    group by state\n",
    "    order by count(*) desc\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55b07d9-d497-408f-9093-69900d04ce07",
   "metadata": {},
   "source": [
    "## Define Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f28d5971-d02f-4c58-861c-897f8954d147",
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_features = [\"bathrooms\", \"bedrooms\", \"security_deposit\", \"cleaning_fee\", \"extra_people\", \"number_of_reviews\", \"square_feet\", \"review_scores_rating\"]\n",
    "categorical_features = [\"room_type\", \"host_is_superhost\", \"cancellation_policy\", \"instant_bookable\", \"state\"]\n",
    "\n",
    "all_features = continuous_features + categorical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9606c5ab-9e4c-47bf-9370-fde6ae31fb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_imputed = datasetImputed.persist()\n",
    "[training_dataset, validation_dataset] = dataset_imputed.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b366ab75-db33-4828-b98a-4462cc603d05",
   "metadata": {},
   "source": [
    "## Create Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "184c67c7-89b0-47ea-8eb3-ec8500cb1e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "continuous_feature_assembler= VectorAssembler(inputCols=continuous_features, outputCol=\"unscaled_continuous_features\")\n",
    "continuous_feature_scaler = StandardScaler(inputCol=\"unscaled_continuous_features\", outputCol=\"scaled_continuous_features\",\\\n",
    "                                           withStd=True, withMean=False)\n",
    "\n",
    "\n",
    "categorical_feature_indexers = [StringIndexer(inputCol=x, outputCol=\"{}_index\".format(x)) for x in categorical_features]\n",
    "#categorical_feature_one_hot_encoders = [OneHotEncoder(inputCol=x.getOutputCol(), outputCol=\"oh_encoder_{}\".format(x.getOutputCol() )) for x in categorical_feature_indexers]\n",
    "\n",
    "\n",
    "estimatorsLr = [continuous_feature_assembler, continuous_feature_scaler] + categorical_feature_indexers #+ categorical_feature_one_hot_encoders\n",
    "\n",
    "#OHE does not work when serializing model\n",
    "\"\"\"\n",
    "Py4JJavaError: An error occurred while calling o1301.serializeToBundle.\n",
    ": java.util.NoSuchElementException: Failed to find a default value for inputCols\n",
    "\tat org.apache.spark.ml.param.Params.$anonfun$getOrDefault$2(params.scala:756)\n",
    "\tat scala.Option.getOrElse(Option.scala:189)\n",
    "\tat org.apache.spark.ml.param.Params.getOrDefault(params.scala:756)\n",
    "\tat org.apache.spark.ml.param.Params.getOrDefault$(params.scala:753)\n",
    "\tat org.apache.spark.ml.PipelineStage.getOrDefault(Pipeline.scala:41)\n",
    "\tat org.apache.spark.ml.param.Params.$(params.scala:762)\n",
    "\tat org.apache.spark.ml.param.Params.$$(params.scala:762)\n",
    "\tat org.apache.spark.ml.PipelineStage.$(Pipeline.scala:41)\n",
    "\tat org.apache.spark.ml.param.shared.HasInputCols.getInputCols(sharedParams.scala:225)\n",
    "\tat org.apache.spark.ml.param.shared.HasInputCols.getInputCols$(sharedParams.scala:225)\n",
    "\tat org.apache.spark.ml.feature.OneHotEncoderModel.getInputCols(OneHotEncoder.scala:226)\n",
    "\tat org.apache.spark.ml.bundle.ops.feature.OneHotEncoderOp$$anon$1.store(OneHotEncoderOp.scala:47)\n",
    "\tat org.apache.spark.ml.bundle.ops.feature.OneHotEncoderOp$$anon$1.store(OneHotEncoderOp.scala:37)\n",
    "\tat ml.combust.bundle.serializer.ModelSerializer.$anonfun$write$1(ModelSerializer.scala:87)\n",
    "\tat scala.util.Try$.apply(Try.scala:213)\n",
    "\tat ml.combust.bundle.serializer.ModelSerializer.write(ModelSerializer.scala:83)\n",
    "\tat ml.combust.bundle.serializer.NodeSerializer.$anonfun$write$1(NodeSerializer.scala:85)\n",
    "\tat scala.util.Try$.apply(Try.scala:213)\n",
    "\tat ml.combust.bundle.serializer.NodeSerializer.write(NodeSerializer.scala:81)\n",
    "\tat ml.combust.bundle.serializer.GraphSerializer.$anonfun$writeNode$1(GraphSerializer.scala:34)\n",
    "\tat scala.util.Try$.apply(Try.scala:213)\n",
    "\tat ml.combust.bundle.serializer.GraphSerializer.writeNode(GraphSerializer.scala:30)\n",
    "\tat ml.combust.bundle.serializer.GraphSerializer.$anonfun$write$2(GraphSerializer.scala:21)\n",
    "\tat scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)\n",
    "\tat scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)\n",
    "\tat scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)\n",
    "\tat ml.combust.bundle.serializer.GraphSerializer.write(GraphSerializer.scala:21)\n",
    "\tat org.apache.spark.ml.bundle.ops.PipelineOp$$anon$1.store(PipelineOp.scala:21)\n",
    "\tat org.apache.spark.ml.bundle.ops.PipelineOp$$anon$1.store(PipelineOp.scala:14)\n",
    "\tat ml.combust.bundle.serializer.ModelSerializer.$anonfun$write$1(ModelSerializer.scala:87)\n",
    "\tat scala.util.Try$.apply(Try.scala:213)\n",
    "\tat ml.combust.bundle.serializer.ModelSerializer.write(ModelSerializer.scala:83)\n",
    "\tat ml.combust.bundle.serializer.NodeSerializer.$anonfun$write$1(NodeSerializer.scala:85)\n",
    "\tat scala.util.Try$.apply(Try.scala:213)\n",
    "\tat ml.combust.bundle.serializer.NodeSerializer.write(NodeSerializer.scala:81)\n",
    "\tat ml.combust.bundle.serializer.GraphSerializer.$anonfun$writeNode$1(GraphSerializer.scala:34)\n",
    "\tat scala.util.Try$.apply(Try.scala:213)\n",
    "\tat ml.combust.bundle.serializer.GraphSerializer.writeNode(GraphSerializer.scala:30)\n",
    "\tat ml.combust.bundle.serializer.GraphSerializer.$anonfun$write$2(GraphSerializer.scala:21)\n",
    "\tat scala.collection.IndexedSeqOptimized.foldLeft(IndexedSeqOptimized.scala:60)\n",
    "\tat scala.collection.IndexedSeqOptimized.foldLeft$(IndexedSeqOptimized.scala:68)\n",
    "\tat scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:38)\n",
    "\tat ml.combust.bundle.serializer.GraphSerializer.write(GraphSerializer.scala:21)\n",
    "\tat org.apache.spark.ml.bundle.ops.PipelineOp$$anon$1.store(PipelineOp.scala:21)\n",
    "\tat org.apache.spark.ml.bundle.ops.PipelineOp$$anon$1.store(PipelineOp.scala:14)\n",
    "\tat ml.combust.bundle.serializer.ModelSerializer.$anonfun$write$1(ModelSerializer.scala:87)\n",
    "\tat scala.util.Try$.apply(Try.scala:213)\n",
    "\tat ml.combust.bundle.serializer.ModelSerializer.write(ModelSerializer.scala:83)\n",
    "\tat ml.combust.bundle.serializer.NodeSerializer.$anonfun$write$1(NodeSerializer.scala:85)\n",
    "\tat scala.util.Try$.apply(Try.scala:213)\n",
    "\tat ml.combust.bundle.serializer.NodeSerializer.write(NodeSerializer.scala:81)\n",
    "\tat ml.combust.bundle.serializer.BundleSerializer.$anonfun$write$1(BundleSerializer.scala:34)\n",
    "\tat scala.util.Try$.apply(Try.scala:213)\n",
    "\tat ml.combust.bundle.serializer.BundleSerializer.write(BundleSerializer.scala:29)\n",
    "\tat ml.combust.bundle.BundleWriter.save(BundleWriter.scala:34)\n",
    "\tat ml.combust.mleap.spark.SimpleSparkSerializer.$anonfun$serializeToBundleWithFormat$4(SimpleSparkSerializer.scala:26)\n",
    "\tat resource.AbstractManagedResource.$anonfun$acquireFor$1(AbstractManagedResource.scala:88)\n",
    "\tat scala.util.control.Exception$Catch.$anonfun$either$1(Exception.scala:252)\n",
    "\tat scala.util.control.Exception$Catch.apply(Exception.scala:228)\n",
    "\tat scala.util.control.Exception$Catch.either(Exception.scala:252)\n",
    "\tat resource.AbstractManagedResource.acquireFor(AbstractManagedResource.scala:88)\n",
    "\tat resource.ManagedResourceOperations.apply(ManagedResourceOperations.scala:26)\n",
    "\tat resource.ManagedResourceOperations.apply$(ManagedResourceOperations.scala:26)\n",
    "\tat resource.AbstractManagedResource.apply(AbstractManagedResource.scala:50)\n",
    "\tat resource.DeferredExtractableManagedResource.$anonfun$tried$1(AbstractManagedResource.scala:33)\n",
    "\tat scala.util.Try$.apply(Try.scala:213)\n",
    "\tat resource.DeferredExtractableManagedResource.tried(AbstractManagedResource.scala:33)\n",
    "\tat ml.combust.mleap.spark.SimpleSparkSerializer.serializeToBundleWithFormat(SimpleSparkSerializer.scala:25)\n",
    "\tat ml.combust.mleap.spark.SimpleSparkSerializer.serializeToBundle(SimpleSparkSerializer.scala:17)\n",
    "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
    "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
    "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
    "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
    "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
    "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
    "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
    "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
    "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
    "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
    "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "featurePipeline = Pipeline(stages=estimatorsLr)\n",
    "\n",
    "sparkFeaturePipelineModel = featurePipeline.fit(dataset_imputed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e314cd-9e30-44ad-b962-aeac9c891ce3",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "391ddc0c-17a4-47ca-8d5c-cf1d00b1d8b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/09/19 18:12:13 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "21/09/19 18:12:13 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "linearRegression = LinearRegression(featuresCol=\"scaled_continuous_features\", labelCol=\"price\", predictionCol=\"price_prediction\", maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "\n",
    "pipeline_lr = [sparkFeaturePipelineModel] + [linearRegression]\n",
    "\n",
    "sparkPipelineEstimatorLr = Pipeline(stages = pipeline_lr)\n",
    "\n",
    "sparkPipelineLr = sparkPipelineEstimatorLr.fit(dataset_imputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bec90a68-2c54-4537-9b6f-2b7165357eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "logisticRegression = LogisticRegression(featuresCol=\"scaled_continuous_features\", labelCol=\"n_bathrooms_more_than_two\", predictionCol=\"n_bathrooms_more_than_two_prediction\", maxIter=10)\n",
    "\n",
    "pipeline_log_r = [sparkFeaturePipelineModel] + [logisticRegression]\n",
    "\n",
    "sparkPipelineEstimatorLogr = Pipeline(stages = pipeline_log_r)\n",
    "\n",
    "sparkPipelineLogr = sparkPipelineEstimatorLogr.fit(dataset_imputed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae6b281-80c1-47d6-915d-f661809b64ea",
   "metadata": {},
   "source": [
    "## Serialize / Deserialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4d6e6db5-3fc6-459e-af4f-de877341443d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "pwd = os.getcwd()\n",
    "\n",
    "sparkPipelineLr.serializeToBundle(f\"jar:file:{pwd}/models/airbnb.lr.zip\", sparkPipelineLr.transform(dataset_imputed))\n",
    "sparkPipelineLogr.serializeToBundle(f\"jar:file:{pwd}/models/airbnb.logr.zip\", dataset=sparkPipelineLogr.transform(dataset_imputed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "84cc6406-05cb-45bf-9ae6-3920d0a1a513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try loading the saved model\n",
    "\n",
    "sparkPipelineLrLoad = PipelineModel.deserializeFromBundle(f\"jar:file:{pwd}/models/airbnb.lr.zip\")\n",
    "sparkPipelineLogrLoad = PipelineModel.deserializeFromBundle(f\"jar:file:{pwd}/models/airbnb.logr.zip\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
